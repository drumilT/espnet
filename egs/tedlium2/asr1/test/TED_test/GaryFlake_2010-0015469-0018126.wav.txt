stage 0: Data preparation
stage 1: Feature Generation
steps/make_fbank_pitch.sh --cmd run.pl --nj 1 --write_utt2num_frames true decode/GaryFlake_2010-0015469-0018126/data decode/GaryFlake_2010-0015469-0018126/log decode/GaryFlake_2010-0015469-0018126/fbank
steps/make_fbank_pitch.sh: moving decode/GaryFlake_2010-0015469-0018126/data/feats.scp to decode/GaryFlake_2010-0015469-0018126/data/.backup
utils/validate_data_dir.sh: WARNING: you have only one speaker.  This probably a bad idea.
   Search for the word 'bold' in http://kaldi-asr.org/doc/data_prep.html
   for more information.
utils/validate_data_dir.sh: Successfully validated data-directory decode/GaryFlake_2010-0015469-0018126/data
steps/make_fbank_pitch.sh: [info]: no segments file exists: assuming wav.scp indexed by utterance.
Succeeded creating filterbank & pitch features for data
/home/drumil/exp/espnet/egs/tedlium2/asr1/../../../utils/dump.sh --cmd run.pl --nj 1 --do_delta false decode/GaryFlake_2010-0015469-0018126/data/feats.scp decode/download/tedlium2.transformer.v1/data/train_trim_sp/cmvn.ark decode/GaryFlake_2010-0015469-0018126/log decode/GaryFlake_2010-0015469-0018126/dump
stage 2: Json Data Preparation
stage 3: Decoding

Recognized text: ▁we▁think▁it▁changes▁the▁way▁information▁can▁be▁used▁so▁i▁want▁to▁extrapolate▁on▁this▁idea▁a▁bit▁with▁something▁that's▁a▁little▁bit▁crazy▁what▁we've▁done▁here▁is▁we've▁taken▁every▁single▁wikipedia▁page▁and▁we▁reduced▁it▁down▁to▁a▁little▁summary▁so▁the▁summary▁consists▁of▁just▁a▁little▁synopsis▁and▁an▁icond▁to▁indicate▁the▁topical▁area▁that▁it▁comes▁from▁i'm▁only▁showing▁the▁top▁five▁hundred▁most▁popular▁wikipedia▁pages▁right▁here▁but▁i'm▁on▁this▁limited▁view

Finished
